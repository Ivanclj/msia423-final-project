{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Conda tensorflow for MAC\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/sample/Churn_Modelling.csv\",index_col='RowNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age',\n",
       "       'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n",
       "       'EstimatedSalary', 'Exited'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "drop = ['CustomerId', 'Surname']\n",
    "data.drop(drop, axis=1, inplace=True)\n",
    "# create dummy for Geography and Gender\n",
    "Geography = pd.get_dummies(data['Geography'], drop_first=True)\n",
    "Gender = pd.get_dummies(data['Gender'], drop_first=True)\n",
    "data.drop(['Geography','Gender'],axis=1,inplace=True)\n",
    "# new dataframe after encode categorical variables as dummies\n",
    "data1 = pd.concat([data,Geography,Gender],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% train and 20% test random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 11)\n",
      "(3000, 11)\n"
     ]
    }
   ],
   "source": [
    "X = data1.drop('Exited', axis=1)\n",
    "y = data1['Exited']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7963\n",
       "1    2037\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crude XGBoost model to select 8 features based on information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(xtrain, ytrain, learnRate=0.1, maxDepth=3, numEstimators=100):\n",
    "    \"\"\"Function to apply crude xgboost algorithm on training data to select 8 features by gain.\n",
    "    \n",
    "    Args:\n",
    "        xtrain (DataFrame): A DataFrame with features.\n",
    "        ytrain (Series): A Series with correct class label.\n",
    "        learnRate (numeric, optional): Boosting learning rate. Default to 0.1.\n",
    "        maxDepth (numeric, optional): Maximum tree depth for base learners. Default to 3.\n",
    "        numEstimators (numeric, optional): Number of boosted trees to fit. Default to 100.\n",
    "    \n",
    "    Returns:\n",
    "        xgb (:obj:`xgboost.sklearn.XGBClassifier`): The xgboost classifier.    \n",
    "        selected_features (str): The features selected from the model.\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize the xgboost classifier\n",
    "    xgb_classifier = xgb.XGBClassifier(objective ='binary:logistic', learning_rate = learnRate,\n",
    "                                       max_depth = maxDepth, n_estimators = numEstimators)\n",
    "    \n",
    "    # fit the classifier on training data\n",
    "    xgb_classifier.fit(xtrain,ytrain)\n",
    "    \n",
    "    # get importance (information gain) for each feature\n",
    "    importance = xgb_classifier.get_booster().get_score(importance_type='gain')\n",
    "    # select 8 features with most gain\n",
    "    selected_features = []\n",
    "    for value in sorted(importance.values(), reverse=True)[:8]:\n",
    "        selected_features.append(list(importance.keys())[list(importance.values()).index(value)])\n",
    "    # print cumulative gain of selected features\n",
    "    cum_gain = 0\n",
    "    for feature in selected_features:\n",
    "        cum_gain += importance[feature]\n",
    "    print('cumulative gain of selected features is {:.4f}'.format(cum_gain/sum(importance.values())))\n",
    "    \n",
    "    return xgb_classifier, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative gain of selected features is 0.9380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'IsActiveMember',\n",
       " 'NumOfProducts',\n",
       " 'Germany',\n",
       " 'Male',\n",
       " 'Balance',\n",
       " 'HasCrCard',\n",
       " 'Tenure']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = feature_selection(X_train,y_train)\n",
    "xgb_bin = result[0]\n",
    "selected_features = result[1]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgboost.sklearn.XGBClassifier"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xgb_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. XGBoost\n",
    "2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_tuning(xtrain, ytrain, n_estimators, learning_rate, max_depth, selected_features):\n",
    "    \"\"\"Function to run xgboost grid search algorithm on training data to select best tuning parameters.\n",
    "    \n",
    "    Args:\n",
    "        xtrain (DataFrame): A DataFrame with features.\n",
    "        ytrain (Series): A Series with correct class label.\n",
    "        num_estimators (list): Number of boosted trees to fit.\n",
    "        learning_rate (list): Boosting learning rates.\n",
    "        max_depth (list): Maximum tree depths for base learners.\n",
    "        selected_features (list): A list of selected features to train the model on.\n",
    "    \n",
    "    Returns:\n",
    "        clf.best_estimator_ (:obj:`xgboost.sklearn.XGBClassifier`): The best xgboost classifier.\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize xgboost classifier\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    # prepare parameters for grid search\n",
    "    param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth)\n",
    "    # run grid search with different combinations of parameters\n",
    "    clf = GridSearchCV(xgb_model, param_grid, n_jobs=5, \n",
    "                       cv=StratifiedKFold(n_splits=5, shuffle=True), \n",
    "                       scoring='roc_auc',\n",
    "                       verbose=2, refit=True)\n",
    "    # fit model on training data\n",
    "    clf.fit(xtrain[selected_features], ytrain)\n",
    "    # return model with best combination of parameters\n",
    "    return clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=5)]: Done 320 out of 320 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [50, 100, 200, 300]\n",
    "learning_rate = [0.001, 0.01, 0.05, 0.1]\n",
    "max_depth = [3, 4, 5, 6]\n",
    "xgb_model = xgboost_tuning(X_train, y_train, n_estimators, learning_rate, max_depth, selected_features)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=300,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)\n",
    "xgb_model.fit(X_train[selected_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_tuning(xtrain, ytrain, n_estimators, max_depth, max_features, selected_features):\n",
    "    \"\"\"Function to run random forest grid search algorithm on training data to select best tuning parameters.\n",
    "    \n",
    "    Args:\n",
    "        xtrain (DataFrame): A DataFrame with features.\n",
    "        ytrain (Series): A Series with correct class label.\n",
    "        num_estimators (list): Number of trees in the forest.\n",
    "        max_depth (list): Maximum tree depths for base learners.\n",
    "        max_features (list): Max number of features considered for splitting a node.\n",
    "        selected_features (list): A list of selected features to train the model on.\n",
    "    \n",
    "    Returns:\n",
    "        rf_cv.best_estimator_ (:obj:`RandomForestClassifier`): The best random forest classifier.\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize xgboost classifier\n",
    "    rf = RandomForestClassifier()\n",
    "    # prepare parameters for grid search\n",
    "    param_grid = dict(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n",
    "    # run grid search with different combinations of parameters\n",
    "    rf_cv = GridSearchCV(rf, param_grid, n_jobs=5, \n",
    "                         cv=StratifiedKFold(n_splits=5, shuffle=True), \n",
    "                         scoring='roc_auc',\n",
    "                         verbose=2, refit=True)\n",
    "    # fit model on training data\n",
    "    rf_cv.fit(xtrain[selected_features], ytrain)\n",
    "    # return model with best combination of parameters\n",
    "    return rf_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed:   56.4s\n",
      "[Parallel(n_jobs=5)]: Done 320 out of 320 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features=4, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [50, 100, 200, 300]\n",
    "max_depth = [3, 4, 5, 6]\n",
    "max_features = [3, 4, 5, 6]\n",
    "rf = random_forest_tuning(X_train, y_train, n_estimators, max_depth, max_features, selected_features)\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features=4, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=6, max_features=4,n_estimators=300)\n",
    "rf.fit(X_train[selected_features],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare AUC and F1 scores across\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model Training\n",
    "def logit(xtrain, ytrain, selected_features):\n",
    "    \"\"\"Function to run logistic regression algorithm on training data.\n",
    "    \n",
    "    Args:\n",
    "        xtrain (DataFrame): A DataFrame with features.\n",
    "        ytrain (Series): A Series with correct class label.\n",
    "        selected_features (list): A list of selected features to train the model on.\n",
    "    \n",
    "    Returns:\n",
    "        logmodel (:obj:`LogisticRegression`): The best xgboost classifier.\n",
    "    \n",
    "    \"\"\"\n",
    "    logmodel = LogisticRegression()\n",
    "    logmodel.fit(X_train[selected_features],y_train)\n",
    "    return logmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianfu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel = logit(X_train, y_train, selected_features)\n",
    "logmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    \"\"\"Function to evaluate model performance on test data.\n",
    "    \n",
    "    Args:\n",
    "        model (:obj: model trained): Classification model trained.\n",
    "        test_features (DataFrame): A DataFrame as test set with selected features.\n",
    "        test_labels (Series): A Series with correct class label for test set.\n",
    "    \n",
    "    \"\"\"\n",
    "    # prediction results on test set\n",
    "    predictions = model.predict(test_features)\n",
    "    predict_probs = model.predict_proba(test_features)[:,1]\n",
    "\n",
    "    print('################### Model Performance ###################')\n",
    "    print(\"F1-score: \" + str(f1_score(y_test, predictions)))\n",
    "    print(\"AUC: \" + str(roc_auc_score(y_test, predict_probs)))\n",
    "    print()\n",
    "    print('################### Confusion Matrix ###################')\n",
    "    print(confusion_matrix(test_labels, predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### Model Performance ###################\n",
      "F1-score: 0.0\n",
      "AUC: 0.30886980631407057\n",
      "\n",
      "################### Confusion Matrix ###################\n",
      "[[2416    0]\n",
      " [ 584    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianfu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate(logmodel, X_test[selected_features], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### Model Performance ###################\n",
      "F1-score: 0.570801317233809\n",
      "AUC: 0.8616199508981222\n",
      "\n",
      "################### Confusion Matrix ###################\n",
      "[[2349   67]\n",
      " [ 324  260]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(rf, X_test[selected_features], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### Model Performance ###################\n",
      "F1-score: 0.598941798941799\n",
      "AUC: 0.8686262530617799\n",
      "\n",
      "################### Confusion Matrix ###################\n",
      "[[2338   78]\n",
      " [ 301  283]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(xgb_model, X_test[selected_features], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_prob(model, xtest, selected_features):\n",
    "    \"\"\"Function to generate predicted class probabilities for test data.\n",
    "    \n",
    "    Args:\n",
    "        model (:obj: model trained): Classification model trained.\n",
    "        xtest (Series): A DataFrame as test set with features.\n",
    "        selected_features (list): A list of selected features on which the model was trained.\n",
    "    \n",
    "    Returns:\n",
    "        churn_probs (numpy.ndarray): Predicted probability of churn for each observation in test.\n",
    "    \n",
    "    \"\"\"\n",
    "    probs = model.predict_proba(xtest[selected_features])\n",
    "    churn_probs = probs[:,1]\n",
    "    return churn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29073487, 0.24275398, 0.21948375, ..., 0.25452591, 0.29221372,\n",
       "       0.2868338 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_probs = predicted_prob(logmodel, X_test, selected_features)\n",
    "logmodel_probs\n",
    "#print(logmodel_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05584555, 0.06982769, 0.12036121, ..., 0.10114341, 0.02369491,\n",
       "       0.24272336])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_probs = predicted_prob(rf, X_test, selected_features)\n",
    "rf_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04434106, 0.03925258, 0.08520908, ..., 0.05524413, 0.01158028,\n",
       "       0.36693183], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_probs = predicted_prob(xgb_model, X_test, selected_features)\n",
    "xgb_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1a26892e80>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(rf_probs.tolist(),columns=['p_churn']).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFOlJREFUeJzt3X+QXeV93/H3x8gYB9kIG3uHCsVyGzmxx4wdvEPIeMZdGU+CoWPxh+ngklgwcjWTEI9bk4wVtzNJ23SqtENpIBknanAsXByZ0jrSYJyEyt66zgQSUTvIgBMEUUCIoDoIOWtwUpxv/7iHZiNW2rO79+6PZ9+vmZ095znPPef7rFafffacc8+mqpAktetlS12AJGm0DHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9NJJkkwm+dBS1yENi0EvSY0z6KURSrJmqWuQDHo1IcnhJD+b5KEkx5P8RpKzZnnNliRfS/KtJI8muWza5jck+b0kf5nkd5Oc171mIsmRGY79nm7555PcmeS/JPkWcG3XdkeS27r9PZhkfNhfA+lUDHq15BrgR4F/ALwJ+Jen6pjkYuA24GeAdcC7gMPTuvwT4Drg9cCZwE/PoY4twJ3dfm/v2t4H7Ona9gG/PIf9SQti0Kslv1xVT1TVM8C/BT5wmr7bgE9W1T1V9TdV9WRVfWPa9t+oqj+pqueBO4C3z6GO36+q3+r2+3zX9pWquruqvgt8GnjbHPYnLYhBr5Y8MW35z4C/d5q+G4BHT7P9z6ctPwesnWcdp9rfWZ6/12Ix6NWSDdOWvxc4epq+TzA4xTNX3wa+58WVJGcArzupj4+E1bJi0Ksl1ye5IMlrgI8Dnz1N31uB65JcmuRlSdYn+YEex/gTBrPxK5K8nMF1gFcsvHRpdAx6teQzwO8Cj3Ufv3CqjlX1Bwwutt4EnAD+J/CG2Q5QVSeAnwR+HXiSwQz/yGlfJC2x+IdH1IIkh4EPVdX/WOpapOXGGb0kNc6gV7OSfDzJ1AwfX1jq2qTF5KkbSWqcM3pJatyyeMPGeeedVxs3bpzXa7/97W9z9tlnD7egZc4xrw6OeXVYyJjvv//+b1bVye/jeIllEfQbN27kwIED83rt5OQkExMTwy1omXPMq4NjXh0WMuYkf9ann6duJKlxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpccvinbELcfDJE1y74/Oz9ju884pFqEaSlp9eM/ok65LcmeQbSR5O8sNJXpPkniSPdJ/P7fomyc1JDiV5IMlFox2CJOl0+p66+SXgt6vqB4C3AQ8DO4D9VbUJ2N+tA7wX2NR9bAc+MdSKJUlzMmvQJ3k18C4Gf0yZqvrrqnoW2ALs7rrtBq7slrcAt9XAvcC6JOcPvXJJUi+z/uGRJG8HdgEPMZjN3w98BHiyqtZN63e8qs5Nchews6q+0rXvBz5WVQdO2u92BjN+xsbG3rFnz555DeDYMyd4+vnZ+124/px57X85mpqaYu3atUtdxqJyzKuDY56bzZs3319V47P163Mxdg1wEfDhqrovyS/xt6dpZpIZ2l7y06SqdjH4AcL4+HjN9zGdt9y+lxsPzj6Mw9fMb//LkY9yXR0c8+qwGGPuc47+CHCkqu7r1u9kEPxPv3hKpvt8bFr/DdNefwFwdDjlSpLmatagr6o/B55I8v1d06UMTuPsA7Z2bVuBvd3yPuCD3d03lwAnquqp4ZYtSeqr7330HwZuT3Im8BhwHYMfEnck2QY8DlzV9b0buBw4BDzX9ZUkLZFeQV9VXwNmOuF/6Qx9C7h+gXVJkobERyBIUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuN6BX2Sw0kOJvlakgNd22uS3JPkke7zuV17ktyc5FCSB5JcNMoBSJJOby4z+s1V9faqGu/WdwD7q2oTsL9bB3gvsKn72A58YljFSpLmbiGnbrYAu7vl3cCV09pvq4F7gXVJzl/AcSRJC5Cqmr1T8qfAcaCAX6uqXUmerap10/ocr6pzk9wF7Kyqr3Tt+4GPVdWBk/a5ncGMn7GxsXfs2bNnXgM49swJnn5+9n4Xrj9nXvtfjqampli7du1Sl7GoHPPq4JjnZvPmzfdPO8tySmt67u+dVXU0yeuBe5J84zR9M0PbS36aVNUuYBfA+Ph4TUxM9Czl77rl9r3ceHD2YRy+Zn77X44mJyeZ79drpXLMq4NjHo1eQV9VR7vPx5J8DrgYeDrJ+VX1VHdq5ljX/QiwYdrLLwCODrHmedm44/O9+x7eecUIK5GkxTXrOfokZyd51YvLwI8AXwf2AVu7bluBvd3yPuCD3d03lwAnquqpoVcuSeqlz4x+DPhckhf7f6aqfjvJHwJ3JNkGPA5c1fW/G7gcOAQ8B1w39KolSb3NGvRV9Rjwthna/wK4dIb2Aq4fSnWSpAXznbGS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1LjeQZ/kjCRfTXJXt/7GJPcleSTJZ5Oc2bW/ols/1G3fOJrSJUl9zGVG/xHg4WnrvwjcVFWbgOPAtq59G3C8qr4PuKnrJ0laIr2CPskFwBXAr3frAd4N3Nl12Q1c2S1v6dbptl/a9ZckLYFU1eydkjuBfwe8Cvhp4Frg3m7WTpINwBeq6q1Jvg5cVlVHum2PAj9UVd88aZ/bge0AY2Nj79izZ8+8BnDsmRM8/fy8XnpKF64/Z7g7HLKpqSnWrl271GUsKse8Ojjmudm8efP9VTU+W781s3VI8o+AY1V1f5KJF5tn6Fo9tv1tQ9UuYBfA+Ph4TUxMnNyll1tu38uNB2cdxpwcvmZ+tSyWyclJ5vv1Wqkc8+rgmEejT0K+E3hfksuBs4BXA/8JWJdkTVW9AFwAHO36HwE2AEeSrAHOAZ4ZeuWSpF5mPUdfVT9bVRdU1UbgauCLVXUN8CXg/V23rcDebnlft063/YvV5/yQJGkkFnIf/ceAjyY5BLwWuLVrvxV4bdf+UWDHwkqUJC3EnE5uV9UkMNktPwZcPEOf7wBXDaE2SdIQ+M5YSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0b7vN9G7Fxx+d79Tu884oRVyJJC+eMXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXGzBn2Ss5L8QZI/SvJgkn/Vtb8xyX1JHkny2SRndu2v6NYPdds3jnYIkqTT6TOj/yvg3VX1NuDtwGVJLgF+EbipqjYBx4FtXf9twPGq+j7gpq6fJGmJzBr0NTDVrb68+yjg3cCdXftu4MpueUu3Trf90iQZWsWSpDnpdY4+yRlJvgYcA+4BHgWeraoXui5HgPXd8nrgCYBu+wngtcMsWpLUX6+/MFVV3wXenmQd8DngzTN16z7PNHuvkxuSbAe2A4yNjTE5OdmnlJcYeyXccOELs3ccgfnWvFBTU1NLduyl4phXB8c8GnP6U4JV9WySSeASYF2SNd2s/QLgaNftCLABOJJkDXAO8MwM+9oF7AIYHx+viYmJeQ3gltv3cuPBpfmLiIevmViS405OTjLfr9dK5ZhXB8c8Gn3uunldN5MnySuB9wAPA18C3t912wrs7Zb3det0279YVS+Z0UuSFkefqfD5wO4kZzD4wXBHVd2V5CFgT5JfAL4K3Nr1vxX4dJJDDGbyV4+gbklST7MGfVU9APzgDO2PARfP0P4d4KqhVCdJWjDfGStJjVuaq5iN2Ljj8736Hd55xYgrkaRTc0YvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuNmDfokG5J8KcnDSR5M8pGu/TVJ7knySPf53K49SW5OcijJA0kuGvUgJEmn1mdG/wJwQ1W9GbgEuD7JW4AdwP6q2gTs79YB3gts6j62A58YetWSpN5mDfqqeqqq/ne3/JfAw8B6YAuwu+u2G7iyW94C3FYD9wLrkpw/9MolSb2kqvp3TjYCXwbeCjxeVeumbTteVecmuQvYWVVf6dr3Ax+rqgMn7Ws7gxk/Y2Nj79izZ8+8BnDsmRM8/fy8XrpoLlx/zlD3NzU1xdq1a4e6z+XOMa8OjnluNm/efH9Vjc/Wb03fHSZZC/w34J9V1beSnLLrDG0v+WlSVbuAXQDj4+M1MTHRt5S/45bb93Ljwd7DWBKHr5kY6v4mJyeZ79drpXLMq4NjHo1ed90keTmDkL+9qv571/z0i6dkus/HuvYjwIZpL78AODqcciVJc9XnrpsAtwIPV9V/nLZpH7C1W94K7J3W/sHu7ptLgBNV9dQQa5YkzUGfcx7vBH4cOJjka13bx4GdwB1JtgGPA1d12+4GLgcOAc8B1w21YknSnMwa9N1F1VOdkL90hv4FXL/Aupqyccfne/U7vPOKEVciaTXynbGS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDVueT8kZpXpe7/9py47e8SVSGqJM3pJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuP8wyMr0MEnT3Btjz9ScnjnFYtQjaTlbtYZfZJPJjmW5OvT2l6T5J4kj3Sfz+3ak+TmJIeSPJDkolEWL0maXZ9TN58CLjupbQewv6o2Afu7dYD3Apu6j+3AJ4ZTpiRpvmYN+qr6MvDMSc1bgN3d8m7gymntt9XAvcC6JOcPq1hJ0tylqmbvlGwE7qqqt3brz1bVumnbj1fVuUnuAnZW1Ve69v3Ax6rqwAz73M5g1s/Y2Ng79uzZM68BHHvmBE8/P6+Xrlhjr6TXmC9cf06v/R188kTvY/fd57BNTU2xdu3aJTn2UnHMq8NCxrx58+b7q2p8tn7DvhibGdpm/ElSVbuAXQDj4+M1MTExrwPecvtebjy4uq4p33DhC73GfPiaiV7763Nhd677HLbJyUnm+z2yUjnm1WExxjzf2yuffvGUTPf5WNd+BNgwrd8FwNH5lydJWqj5Bv0+YGu3vBXYO639g93dN5cAJ6rqqQXWKElagFl//0/ym8AEcF6SI8DPATuBO5JsAx4Hruq63w1cDhwCngOuG0HNkqQ5mDXoq+oDp9h06Qx9C7h+oUVJkobHRyBIUuNW1+0qq8zGOdxNI6ldzuglqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqc99FrToZ9b75/7lAaPWf0ktQ4g16SGmfQS1LjDHpJapwXY7Wk+l7c/dRlZy/Jcb1YrPlaqu/tmTijl6TGOaNXU4Z9++dc9ufsv30r9dHfBr1WhINPnuDaFfqfTFpqnrqRpMY5o5eGZLlf4PVdzauXQS8tsr6Be8OFL/Q6XWXgntpy/+G7WAx6SSM37FsNl/Ki+0rkOXpJapwzemmFW6rZ6CiO691Vo+GMXpIaN5KgT3JZkj9OcijJjlEcQ5LUz9CDPskZwK8A7wXeAnwgyVuGfRxJUj+jmNFfDByqqseq6q+BPcCWERxHktRDqmq4O0zeD1xWVR/q1n8c+KGq+qmT+m0Htner3w/88TwPeR7wzXm+dqVyzKuDY14dFjLmN1TV62brNIq7bjJD20t+mlTVLmDXgg+WHKiq8YXuZyVxzKuDY14dFmPMozh1cwTYMG39AuDoCI4jSephFEH/h8CmJG9MciZwNbBvBMeRJPUw9FM3VfVCkp8Cfgc4A/hkVT047ONMs+DTPyuQY14dHPPqMPIxD/1irCRpefGdsZLUOINekhq3IoJ+tkcqJHlFks922+9LsnHxqxyuHmP+aJKHkjyQZH+SNyxFncPW9/EZSd6fpJKs+Fvx+ow5yT/u/r0fTPKZxa5x2Hp8f39vki8l+Wr3PX75UtQ5LEk+meRYkq+fYnuS3Nx9PR5IctFQC6iqZf3B4ILuo8DfB84E/gh4y0l9fhL41W75auCzS133Iox5M/A93fJPrPQx9x131+9VwJeBe4Hxpa57Ef6tNwFfBc7t1l+/1HUvwph3AT/RLb8FOLzUdS9wzO8CLgK+fortlwNfYPA+pEuA+4Z5/JUwo+/zSIUtwO5u+U7g0iQzvXFrpZh1zFX1pap6rlu9l8H7FVa6vo/P+DfAvwe+s5jFjUifMf9T4Feq6jhAVR1b5BqHrc+YC3h1t3wOK/y9OFX1ZeCZ03TZAtxWA/cC65KcP6zjr4SgXw88MW39SNc2Y5+qegE4Abx2UaobjT5jnm4bg9nASjfruJP8ILChqu5azMJGqM+/9ZuANyX5vST3Jrls0aobjT5j/nngx5IcAe4GPrw4pS2Zuf6fn5OV8IdH+jxSoddjF1aQ3uNJ8mPAOPAPR1rR4jjtuJO8DLgJuHaxCloEff6t1zA4fTPB4De3/5XkrVX17IhrG5U+Y/4A8KmqujHJDwOf7sb8N6Mvb0mMNMNWwoy+zyMV/n+fJGsY/Kp3ul+Tlrtej5FI8h7gXwDvq6q/WqTaRmm2cb8KeCswmeQwg3OZ+1b4Bdm+3997q+r/VtWfMngA4KZFqm8U+ox5G3AHQFX9PnAWg4d/tWqkj45ZCUHf55EK+4Ct3fL7gS9Wd4VjhZp1zN0pjF9jEPIr/Zzti0477qo6UVXnVdXGqtrI4NrE+6rqwNKUOxR9vr9/i8HFd5Kcx+BUzmOLWuVw9Rnz48ClAEnezCDo/8+iVrm49gEf7O6+uQQ4UVVPDWvny/7UTZ3ikQpJ/jVwoKr2Abcy+NXuEIOZ/NVLV/HC9RzzfwDWAv+1u+78eFW9b8mKHoKe425KzzH/DvAjSR4Cvgv8TFX9xdJVvTA9x3wD8J+T/HMGpzCuXcmTtyS/yeDU23nddYefA14OUFW/yuA6xOXAIeA54LqhHn8Ff+0kST2shFM3kqQFMOglqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4/4fy+5pHXEZ1ugAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(xgb_probs.tolist(),columns=['p_churn']).hist(bins=30)\n",
    "plt.x_label = 'predicted probability of churn'\n",
    "plt.y_label = 'count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_features_input(Age,IsActiveMember,NumOfProducts,Germany,Gender,Balance,HasCrCard,Tenure):\n",
    "    \"\"\"Function to handle user inputs of features.\n",
    "    \n",
    "    Args:\n",
    "        Age (str): Age of the customer.\n",
    "        IsActiveMember (str): Whether or not the customer is an active member.\n",
    "        NumOfProducts (str): Number of bank products the customer is using.\n",
    "        Germany (str): Whether or not the customer is a German customer.\n",
    "        Gender (str): Gender of the customer.\n",
    "        Balance (str): Balance left on customer's account.\n",
    "        HasCrCard (str): Whether or not the customer has a credit card.\n",
    "        Tenure (str): How long the customer has been with the bank.\n",
    "        \n",
    "    Returns:\n",
    "        inputs (list): A list of input features.\n",
    "    \n",
    "    \"\"\"\n",
    "    # valid inputs for binary variables\n",
    "    valid_binary = [\"0\",\"1\"]\n",
    "    \n",
    "    inputs = []\n",
    "    \n",
    "    # Age - int\n",
    "    try:\n",
    "        age = int(Age)\n",
    "        if age >= 0 and age <= 110:\n",
    "            inputs.append(age)\n",
    "        else:\n",
    "            return 'Invalid input for Age, must be an integer between 0 and 110!'\n",
    "    except:\n",
    "        return 'Please input a numeric value!'\n",
    "    \n",
    "    # IsActiveMember - binary\n",
    "    try:\n",
    "        if IsActiveMember in valid_binary:\n",
    "            inputs.append(int(IsActiveMember))\n",
    "        else:\n",
    "            return 'Invalid input for IsActiveMember, must be either 0 or 1!'\n",
    "    except:\n",
    "        return 'Please input a numeric value, either 0 or 1!'\n",
    "    \n",
    "    # NumOfProducts - int\n",
    "    try:\n",
    "        numofproducts = int(NumOfProducts)\n",
    "        if numofproducts >= 0:\n",
    "            inputs.append(numofproducts)\n",
    "        else:\n",
    "            return 'Invalid input for NumOfProducts, must be an integer greater than 0!'\n",
    "    except:\n",
    "        return 'Please input a numeric value!'\n",
    "    \n",
    "    # Germany - binary\n",
    "    try:\n",
    "        if Germany in valid_binary:\n",
    "            inputs.append(int(Germany))\n",
    "        else:\n",
    "            return 'Invalid input for Germany, must be either 0 or 1!'\n",
    "    except:\n",
    "        return 'Please input a numeric value, either 0 or 1!'\n",
    "    \n",
    "    # Gender - binary - Male is 1\n",
    "    try:\n",
    "        if Gender in valid_binary:\n",
    "            inputs.append(int(Gender))\n",
    "        else:\n",
    "            return 'Invalid input for Gender, must be either 0 or 1!'\n",
    "    except:\n",
    "        return 'Please input your gender!'\n",
    "    \n",
    "    # Balance - float\n",
    "    try:\n",
    "        balance = float(Balance)\n",
    "        if balance >= 0:\n",
    "            inputs.append(balance)\n",
    "        else:\n",
    "            return 'Invalid input for balance, must be nonnegative!'\n",
    "    except:\n",
    "        return 'Please input a numeric value!'\n",
    "    \n",
    "    # HasCrCard - binary\n",
    "    try:\n",
    "        if HasCrCard in valid_binary:\n",
    "            inputs.append(int(HasCrCard))\n",
    "        else:\n",
    "            return 'Invalid input for HasCrCard, must be either 0 or 1!'\n",
    "    except:\n",
    "        return 'Please input a numeric value, either 0 or 1!'\n",
    "    \n",
    "    # Tenure - int\n",
    "    try:\n",
    "        tenure = int(Tenure)\n",
    "        if tenure >= 0:\n",
    "            inputs.append(tenure)\n",
    "        else:\n",
    "            return 'Invalid input for Tenure, must be an integer greater than 0!'\n",
    "    except:\n",
    "        return 'Please input a numeric value!'\n",
    "    \n",
    "    return inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
